{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4e5b3e77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: Auto-reload external modules\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4b93cb8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %reload_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "95b01fd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# chatbot_core.py\n",
    "\n",
    "import os\n",
    "import pickle\n",
    "import joblib\n",
    "import torch\n",
    "import numpy as np\n",
    "import csv\n",
    "from datetime import datetime\n",
    "\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from functools import lru_cache\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "307e57ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "project_root = os.path.abspath(\"..\")\n",
    "if project_root not in sys.path:\n",
    "    sys.path.append(project_root)\n",
    "\n",
    "# models_path = os.path.abspath(\"../models\")\n",
    "# if models_path not in sys.path:\n",
    "#     sys.path.append(models_path)\n",
    "\n",
    "from utils.intent_mapping import map_intent_conservative_contextual, intent_keywords\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d03dbe25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Model loader functions (cached)\n",
    "\n",
    "from functools import lru_cache\n",
    "\n",
    "@lru_cache(maxsize=4)\n",
    "def load_logreg():\n",
    "    model = joblib.load(\"saved_models/logreg/logreg_model.pkl\")\n",
    "    vectorizer = joblib.load(\"saved_models/logreg/logreg_vectorizer.pkl\")\n",
    "    label_encoder = joblib.load(\"saved_models/logreg/logreg_label_encoder.pkl\")\n",
    "    print(\"Logistic Regression loaded\")\n",
    "    return model, vectorizer, label_encoder\n",
    "\n",
    "\n",
    "@lru_cache(maxsize=4)\n",
    "def load_roberta():\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\"saved_models/roberta/\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"saved_models/roberta/\")\n",
    "    with open(\"saved_models/roberta/label_encoder.pkl\", \"rb\") as f:\n",
    "        label_encoder = pickle.load(f)\n",
    "    print(\"RoBERTa model loaded\")\n",
    "    return model, tokenizer, label_encoder\n",
    "\n",
    "\n",
    "@lru_cache(maxsize=4)\n",
    "def load_deberta():\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\"saved_models/deberta/\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"saved_models/deberta/\")\n",
    "    with open(\"saved_models/deberta/label_encoder.pkl\", \"rb\") as f:\n",
    "        label_encoder = pickle.load(f)\n",
    "    print(\"DeBERTa model loaded\")\n",
    "    return model, tokenizer, label_encoder\n",
    "\n",
    "\n",
    "@lru_cache(maxsize=4)\n",
    "def load_rnn():\n",
    "    import torch.nn as nn\n",
    "\n",
    "    class BiLSTMWithAttention(nn.Module):\n",
    "        def __init__(self, embedding_matrix, hidden_dim, output_dim, dropout=0.3, num_layers=2):\n",
    "            super().__init__()\n",
    "            vocab_size, embed_dim = embedding_matrix.shape\n",
    "            self.embedding = nn.Embedding.from_pretrained(embedding_matrix, freeze=False, padding_idx=0)\n",
    "            self.lstm = nn.LSTM(embed_dim, hidden_dim, num_layers=num_layers, batch_first=True, bidirectional=True, dropout=dropout)\n",
    "            self.attention = nn.Linear(hidden_dim * 2, 1)\n",
    "            self.dropout = nn.Dropout(dropout)\n",
    "            self.fc = nn.Linear(hidden_dim * 2, output_dim)\n",
    "\n",
    "        def forward(self, x):\n",
    "            embedded = self.embedding(x)\n",
    "            lstm_out, _ = self.lstm(embedded)\n",
    "            attn_weights = torch.softmax(self.attention(lstm_out), dim=1)\n",
    "            context = torch.sum(attn_weights * lstm_out, dim=1)\n",
    "            return self.fc(context)\n",
    "\n",
    "    with open(\"saved_models/rnn/vocab.pkl\", \"rb\") as f:\n",
    "        vocab = pickle.load(f)\n",
    "\n",
    "    with open(\"saved_models/rnn/label_encoder.pkl\", \"rb\") as f:\n",
    "        label_encoder = pickle.load(f)\n",
    "\n",
    "    embedding_matrix = np.load(\"saved_models/rnn/embedding_matrix.npy\") \\\n",
    "        if os.path.exists(\"saved_models/rnn/embedding_matrix.npy\") \\\n",
    "        else np.random.normal(0, 1, (len(vocab), 100)).astype(np.float32)\n",
    "\n",
    "    model = BiLSTMWithAttention(\n",
    "        embedding_matrix=torch.tensor(embedding_matrix),\n",
    "        hidden_dim=128,\n",
    "        output_dim=len(label_encoder.classes_)\n",
    "    )\n",
    "    model.load_state_dict(torch.load(\"saved_models/rnn/rnn_model.pt\", map_location=torch.device(\"cpu\")))\n",
    "    model.eval()\n",
    "\n",
    "    print(\"RNN model loaded\")\n",
    "    return model, vocab, label_encoder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "053c2080",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3A: Rule-based matching (top of predict_intent)\n",
    "from utils.intent_mapping import map_intent_conservative_contextual, intent_keywords\n",
    "from datetime import datetime\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import csv\n",
    "import joblib\n",
    "import pickle\n",
    "import torch\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "from functools import lru_cache\n",
    "\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "#Create Logging Function\n",
    "# ---- Logging Function ----\n",
    "\n",
    "def log_prediction(user_input, intent, model_name, log_path=\"logs/prediction_logs.csv\"):\n",
    "    os.makedirs(os.path.dirname(log_path), exist_ok=True)\n",
    "    timestamp = datetime.now().isoformat()\n",
    "    with open(log_path, mode=\"a\", newline=\"\", encoding=\"utf-8\") as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow([timestamp, user_input, intent, model_name])\n",
    "    return timestamp\n",
    "\n",
    "def predict_intent(user_input):\n",
    "    timestamp = datetime.now().isoformat()\n",
    "    \"\"\"\n",
    "    Predicts the intent of the user_input using a model pipeline.\n",
    "    Returns: (predicted_intent, model_name_used)\n",
    "    \"\"\"\n",
    "\n",
    "    # STEP 1: Rule-based intent detection\n",
    "    rule_intent = map_intent_conservative_contextual(user_input, intent_keywords)\n",
    "\n",
    "    if rule_intent not in [\"OTHER\", \"AMBIGUOUS\"]:\n",
    "        print(\"Rule-based match:\", rule_intent)\n",
    "        log_prediction(user_input, rule_intent, \"rule_based\", confidence=None)\n",
    "        return rule_intent, \"rule_based\", None, timestamp\n",
    "\n",
    "    # Prepare to collect model results\n",
    "    model_results = []\n",
    "\n",
    "    # STEP 2: Logistic Regression (with confidence threshold)\n",
    "    try:\n",
    "        logreg_model, logreg_vectorizer, logreg_label_encoder = load_logreg()\n",
    "        X_input = logreg_vectorizer.transform([user_input])\n",
    "        probs = logreg_model.predict_proba(X_input)\n",
    "        max_prob = probs.max()\n",
    "        y_pred = logreg_model.predict(X_input)\n",
    "        intent = logreg_label_encoder.inverse_transform(y_pred)[0]\n",
    "\n",
    "        print(f\"Logistic Regression predicted: {intent} (confidence: {max_prob:.2f})\")\n",
    "\n",
    "        if max_prob >= 0.7 and intent != \"OTHER\":\n",
    "            log_prediction(user_input, intent, \"logreg\", max_prob)\n",
    "            return intent, \"logreg\", max_prob, timestamp\n",
    "        else:\n",
    "            model_results.append((\"logreg\", \"OTHER\"))\n",
    "\n",
    "    except Exception as e:\n",
    "        print(\"Logistic Regression failed:\", e)\n",
    "\n",
    "    # STEP 3: RoBERTa (with confidence threshold)\n",
    "    try:\n",
    "        roberta_model, roberta_tokenizer, roberta_label_encoder = load_roberta()\n",
    "        inputs = roberta_tokenizer(user_input, return_tensors=\"pt\", padding=True, truncation=True, max_length=128)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = roberta_model(**inputs)\n",
    "            probs = torch.softmax(outputs.logits, dim=1)\n",
    "            max_prob = probs.max().item()\n",
    "            pred_label = torch.argmax(probs, dim=1).item()\n",
    "\n",
    "        intent = roberta_label_encoder.inverse_transform([pred_label])[0]\n",
    "        print(f\"RoBERTa predicted: {intent} (confidence: {max_prob:.2f})\")\n",
    "\n",
    "        if max_prob >= 0.75 and intent != \"OTHER\":\n",
    "            log_prediction(user_input, intent, \"roberta\", max_prob)\n",
    "            return intent, \"roberta\", max_prob, timestamp\n",
    "        else:\n",
    "            model_results.append((\"roberta\", \"OTHER\"))\n",
    "\n",
    "    except Exception as e:\n",
    "        print(\"RoBERTa failed:\", e)\n",
    "    \n",
    "    # STEP 4: DeBERTa (with confidence threshold)\n",
    "    try:\n",
    "        deberta_model, deberta_tokenizer, deberta_label_encoder = load_deberta()\n",
    "        inputs = deberta_tokenizer(user_input, return_tensors=\"pt\", padding=True, truncation=True, max_length=128)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = deberta_model(**inputs)\n",
    "            probs = torch.softmax(outputs.logits, dim=1)\n",
    "            max_prob = probs.max().item()\n",
    "            pred_label = torch.argmax(probs, dim=1).item()\n",
    "\n",
    "        intent = deberta_label_encoder.inverse_transform([pred_label])[0]\n",
    "        print(f\"DeBERTa predicted: {intent} (confidence: {max_prob:.2f})\")\n",
    "\n",
    "        if max_prob >= 0.75 and intent != \"OTHER\":\n",
    "            log_prediction(user_input, intent, \"deberta\", max_prob)\n",
    "            return intent, \"deberta\", max_prob, timestamp\n",
    "        else:\n",
    "            model_results.append((\"deberta\", \"OTHER\"))\n",
    "\n",
    "    except Exception as e:\n",
    "        print(\"DeBERTa failed:\", e)\n",
    "\n",
    "    # STEP 5: RNN (BiLSTM + Attention, with confidence threshold)\n",
    "    try:\n",
    "        rnn_model, rnn_vocab, rnn_label_encoder = load_rnn()\n",
    "\n",
    "        def simple_tokenizer(text):\n",
    "            return text.lower().split()\n",
    "\n",
    "        def text_to_sequence(text, vocab):\n",
    "            return [vocab.get(token, vocab[\"<UNK>\"]) for token in simple_tokenizer(text)]\n",
    "\n",
    "        sequence = text_to_sequence(user_input, rnn_vocab)\n",
    "        sequence_tensor = torch.tensor(sequence, dtype=torch.long).unsqueeze(0)  # shape: (1, seq_len)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            rnn_model.eval()\n",
    "            output = rnn_model(sequence_tensor)\n",
    "            probs = torch.softmax(output, dim=1)\n",
    "            max_prob = probs.max().item()\n",
    "            pred_label = torch.argmax(probs, dim=1).item()\n",
    "\n",
    "        intent = rnn_label_encoder.inverse_transform([pred_label])[0]\n",
    "        print(f\"RNN predicted: {intent} (confidence: {max_prob:.2f})\")\n",
    "\n",
    "        if max_prob >= 0.75 and intent != \"OTHER\":\n",
    "            log_prediction(user_input, intent, \"rnn\", max_prob)\n",
    "            return intent, \"rnn\", max_prob, timestamp\n",
    "        else:\n",
    "            model_results.append((\"rnn\", \"OTHER\"))\n",
    "\n",
    "    except Exception as e:\n",
    "        print(\"RNN failed:\", e)\n",
    "\n",
    "\n",
    "    # STEP 6: Final fallback\n",
    "    print(\"All models returned 'OTHER' or failed.\")\n",
    "\n",
    "    clarification_prompt = (\n",
    "        \"I'm not sure I understood that. \"\n",
    "        \"Could you rephrase your request or would you like to connect with a support agent?\"\n",
    "    )\n",
    "    log_prediction(user_input, \"OTHER\", \"fallback\", confidence=None)\n",
    "    return clarification_prompt, \"fallback\", None, timestamp\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2eb5fefe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rule-based match: GREETINGS\n",
      "('GREETINGS', 'rule_based', None)\n",
      "Rule-based match: PRODUCT_SEARCH\n",
      "('PRODUCT_SEARCH', 'rule_based', None)\n",
      "Logistic Regression predicted: PRODUCT_SEARCH (confidence: 0.12)\n",
      "RoBERTa model loaded\n",
      "RoBERTa predicted: SENTIMENT_PRAISE (confidence: 0.25)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\abhay\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\convert_slow_tokenizer.py:564: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DeBERTa model loaded\n",
      "DeBERTa predicted: VIEW_PRODUCT_DETAILS (confidence: 0.37)\n",
      "RNN model loaded\n",
      "RNN predicted: SENTIMENT_PRAISE (confidence: 0.16)\n",
      "All models returned 'OTHER' or failed.\n",
      "(\"I'm not sure I understood that. Could you rephrase your request or would you like to connect with a support agent?\", 'fallback', None)\n"
     ]
    }
   ],
   "source": [
    "print(predict_intent(\"hello there\"))         # Expect: (\"GREETINGS\", \"rule_based\", None)\n",
    "print(predict_intent(\"I want to buy a phone\"))  # Expect: (some_intent, model_name, confidence)\n",
    "print(predict_intent(\"skdjslkslskdj\"))        # Expect: (fallback_message, \"fallback\", None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a942d086",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last log entry: 2025-07-21T08:53:48.106299,skdjslkslskdj,OTHER,fallback,\n"
     ]
    }
   ],
   "source": [
    "# output = predict_intent(\"hello there\")\n",
    "# print(\"Output:\", output)\n",
    "\n",
    "# Check CSV log manually\n",
    "with open(\"logs/prediction_logs.csv\", \"r\", encoding=\"utf-8\") as f:\n",
    "    lines = f.readlines()\n",
    "    print(\"Last log entry:\", lines[-1].strip())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fc7222ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression loaded\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(LogisticRegression(C=0.1, class_weight='balanced', max_iter=1000,\n",
       "                    random_state=42),\n",
       " TfidfVectorizer(max_features=5000, ngram_range=(1, 2), stop_words='english'),\n",
       " LabelEncoder())"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_logreg.cache_clear()\n",
    "load_logreg()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbc600ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ RoBERTa model loaded\n"
     ]
    }
   ],
   "source": [
    "_ = load_roberta()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11d039dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\abhay\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\convert_slow_tokenizer.py:564: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ DeBERTa model loaded\n"
     ]
    }
   ],
   "source": [
    "_ = load_deberta()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9b2a9e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ RNN model loaded\n"
     ]
    }
   ],
   "source": [
    "_ = load_rnn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38114f2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression loaded\n",
      "Logistic Regression predicted: PRODUCT_SEARCH (confidence: 0.12)\n",
      "RoBERTa model loaded\n",
      "RoBERTa predicted: GOODBYE (confidence: 0.70)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\abhay\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\convert_slow_tokenizer.py:564: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DeBERTa model loaded\n",
      "DeBERTa predicted: GOODBYE (confidence: 0.83)\n",
      "('GOODBYE', 'deberta')\n"
     ]
    }
   ],
   "source": [
    "print(predict_intent(\"hello there\"))  # Expect GREETINGS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4598a4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rule-based match: PRODUCT_SEARCH\n",
      "('PRODUCT_SEARCH', 'rule_based')\n"
     ]
    }
   ],
   "source": [
    "print(predict_intent(\"I want to buy a laptop\"))  # Expect VIEW_PRODUCT_DETAILS or similar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "770f8d07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression predicted: PRODUCT_SEARCH (confidence: 0.12)\n",
      "RoBERTa predicted: PRODUCT_SEARCH (confidence: 0.47)\n",
      "DeBERTa predicted: SENTIMENT_PRAISE (confidence: 0.63)\n",
      "RNN predicted: SENTIMENT_PRAISE (confidence: 0.16)\n",
      "All models returned 'OTHER' or failed.\n",
      "(\"I'm not sure I understood that. Could you rephrase your request or would you like to connect with a support agent?\", 'fallback')\n"
     ]
    }
   ],
   "source": [
    "print(predict_intent(\"sdkjfhsdjkfh\"))  # Expect fallback message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6d1b09b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "GREETINGS\n"
     ]
    }
   ],
   "source": [
    "# %load_ext autoreload\n",
    "# %autoreload 2\n",
    "# from utils.intent_mapping import map_intent_conservative_contextual, intent_keywords\n",
    "# print(map_intent_conservative_contextual(\"hi\", intent_keywords))  # should return GREETINGS\n",
    "\n",
    "# from utils.intent_mapping import map_intent_conservative_contextual, intent_keywords\n",
    "\n",
    "# print(map_intent_conservative_contextual(\"hello there\", intent_keywords))\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
